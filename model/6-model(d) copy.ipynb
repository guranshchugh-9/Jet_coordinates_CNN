{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c727518e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"transformer_encoder\" (type TransformerEncoder).\n\nin user code:\n\n    File \"/var/folders/40/ns22tb8s2kv6817h3bxtgnyr0000gn/T/ipykernel_37525/1825824979.py\", line 20, in call  *\n        return self.layernorm2(out1 + dense_output)\n\n    ValueError: Dimensions must be equal, but are 16 and 32 for '{{node transformer_encoder/add_1}} = AddV2[T=DT_FLOAT](transformer_encoder/layer_normalization/add, transformer_encoder/dense/Relu)' with input shapes: [?,13,16], [?,13,32].\n\n\nCall arguments received by layer \"transformer_encoder\" (type TransformerEncoder):\n  • inputs=tf.Tensor(shape=(None, 13, 16), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(inputs\u001b[38;5;241m=\u001b[39m[signal_input, code_input], outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# 2. Compile with Enhanced Settings -------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m     55\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m     56\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# 3. Advanced Data Augmentation -----------------------------------------------\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m code_input \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m,))\n\u001b[1;32m     35\u001b[0m y \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mEmbedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)(code_input)\n\u001b[0;32m---> 36\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m y \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling1D()(y)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Gated Fusion\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/40/ns22tb8s2kv6817h3bxtgnyr0000gn/T/__autograph_generated_filevmind7m7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mlayernorm2, (ag__\u001b[38;5;241m.\u001b[39mld(out1) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(dense_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"transformer_encoder\" (type TransformerEncoder).\n\nin user code:\n\n    File \"/var/folders/40/ns22tb8s2kv6817h3bxtgnyr0000gn/T/ipykernel_37525/1825824979.py\", line 20, in call  *\n        return self.layernorm2(out1 + dense_output)\n\n    ValueError: Dimensions must be equal, but are 16 and 32 for '{{node transformer_encoder/add_1}} = AddV2[T=DT_FLOAT](transformer_encoder/layer_normalization/add, transformer_encoder/dense/Relu)' with input shapes: [?,13,16], [?,13,32].\n\n\nCall arguments received by layer \"transformer_encoder\" (type TransformerEncoder):\n  • inputs=tf.Tensor(shape=(None, 13, 16), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dense_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense = layers.Dense(dense_dim, activation=\"relu\")\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        dense_output = self.dense(out1)\n",
    "        return self.layernorm2(out1 + dense_output)\n",
    "\n",
    "# 1. Enhanced Architecture ----------------------------------------------------\n",
    "def create_model():\n",
    "    # Signal Branch with Causal Conv + Attention\n",
    "    signal_input = layers.Input(shape=(208, 2))\n",
    "    x = layers.Conv1D(64, 5, activation='relu', padding='causal')(signal_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 3, activation='relu', padding='causal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Attention()([x, x])  # Temporal self-attention\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Code Branch with Transformer\n",
    "    code_input = layers.Input(shape=(13,))\n",
    "    y = layers.Embedding(input_dim=1000, output_dim=16)(code_input)\n",
    "    y = TransformerEncoder(embed_dim=16, num_heads=2, dense_dim=32)(y)\n",
    "    y = layers.GlobalAveragePooling1D()(y)\n",
    "\n",
    "    # Gated Fusion\n",
    "    gate = layers.Dense(128, activation='sigmoid')(x)\n",
    "    gated_y = layers.Multiply()([y, gate])\n",
    "    merged = layers.concatenate([x, gated_y])\n",
    "\n",
    "    # Final Layers\n",
    "    z = layers.Dense(256, activation='relu', kernel_regularizer='l1_l2(0.01,0.01)')(merged)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    outputs = layers.Dense(6, activation='linear')(z)\n",
    "\n",
    "    return Model(inputs=[signal_input, code_input], outputs=outputs)\n",
    "\n",
    "# 2. Compile with Enhanced Settings -------------------------------------------\n",
    "model = create_model()\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=lambda y_true, y_pred: tf.reduce_mean(tf.square(y_true - y_pred) * [1,1,2,1,1,2]),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 3. Advanced Data Augmentation -----------------------------------------------\n",
    "def radar_augment(signal):\n",
    "    signal = tf.roll(signal, shift=np.random.randint(-10,10), axis=1)\n",
    "    signal += tf.random.normal(tf.shape(signal), stddev=0.03)\n",
    "    return signal\n",
    "\n",
    "# 4. Training Protocol --------------------------------------------------------\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_sig_train, X_sec_train))\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda s,c: (radar_augment(s), c)).batch(32).prefetch(2)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=50,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=([X_sig_val, X_sec_val], y_val),\n",
    "    epochs=300,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 5. Post-Training Quantization -----------------------------------------------\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_model = converter.convert()\n",
    "\n",
    "# Save for deployment\n",
    "with open('jet_interceptor.tflite', 'wb') as f:\n",
    "    f.write(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb9c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../dataset/dataset.csv')\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Data Loading & Parsing\n",
    "# ---------------------------\n",
    "def parse_signal(signal_str):\n",
    "    \"\"\"Convert complex string to magnitude/phase features\"\"\"\n",
    "    cleaned = signal_str.strip('[]').replace(' ', '')\n",
    "    parts = cleaned.split('),(')\n",
    "    signal = []\n",
    "    for p in parts:\n",
    "        p = p.replace('(', '').replace(')', '')\n",
    "        try:\n",
    "            cmplx = complex(p)\n",
    "            signal.append([abs(cmplx), np.angle(cmplx)])  # Magnitude + Phase\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return np.array(signal[:208])  # Shape: (208, 2)\n",
    "\n",
    "def parse_secret_code(code_str):\n",
    "    \"\"\"Convert secret code string to integer array\"\"\"\n",
    "    return np.array([int(x) for x in code_str.strip('[]').split(', ')])\n",
    "\n",
    "\n",
    "# Parse all columns\n",
    "X_signal = np.array([parse_signal(s) for s in df['received_signal']])  # (15000, 208, 2)\n",
    "X_secret = np.array([parse_secret_code(c) for c in df['secret_code']])  # (15000, 13)\n",
    "y = df[['jet1_x', 'jet1_y', 'jet1_z', 'jet2_x', 'jet2_y', 'jet2_z']].values  # (15000, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9e3172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.2229913  -1.00620656]\n",
      "  [ 1.02854609 -0.58463834]\n",
      "  [ 1.33988282 -3.05400802]\n",
      "  [ 0.95439686 -2.12703261]\n",
      "  [ 2.39794737 -2.85318375]\n",
      "  [ 2.41176324 -2.94039406]\n",
      "  [ 1.88649359 -2.73590844]\n",
      "  [ 2.09875628  3.05424141]\n",
      "  [ 1.75956874  2.84119159]\n",
      "  [ 2.08312458  3.0230854 ]\n",
      "  [ 2.33714127  2.83773895]\n",
      "  [ 1.91157554  2.64836719]\n",
      "  [ 1.00369649  2.98933202]\n",
      "  [ 1.71609921  2.66619113]\n",
      "  [ 2.39077512  2.67200443]\n",
      "  [ 1.92979245  2.49660498]\n",
      "  [ 2.18216289  2.3755062 ]\n",
      "  [ 1.60648387  2.39479077]\n",
      "  [ 1.56592539  1.85281706]\n",
      "  [ 2.41254537  1.72659667]\n",
      "  [ 1.8774701   1.67987811]\n",
      "  [ 2.01993824  1.74278086]\n",
      "  [ 2.37840474  1.6988247 ]\n",
      "  [ 1.9292007   1.15051429]\n",
      "  [ 1.94939897  1.63812123]\n",
      "  [ 2.18241022  1.13626155]\n",
      "  [ 2.01916819  1.27066915]\n",
      "  [ 1.58849702  1.27336048]\n",
      "  [ 1.73513605  1.39274444]\n",
      "  [ 1.85123951  1.25285589]\n",
      "  [ 2.0786206   0.8912264 ]\n",
      "  [ 2.23254926  0.93281102]\n",
      "  [ 2.61655436  0.78950846]\n",
      "  [ 3.44359385  0.7027852 ]\n",
      "  [ 3.95310036  0.27192271]\n",
      "  [ 3.16634329  0.50931978]\n",
      "  [ 2.57479628  0.26283066]\n",
      "  [ 2.91539126  0.1526312 ]\n",
      "  [ 2.39153297  0.08935632]\n",
      "  [ 2.80662909 -0.24032811]\n",
      "  [ 1.40091349 -0.09018317]\n",
      "  [ 0.87463304 -0.40338947]\n",
      "  [ 0.40528537  0.94457523]\n",
      "  [ 0.86889224  2.84603084]\n",
      "  [ 2.33730497  2.6679907 ]\n",
      "  [ 2.37424236  2.80752044]\n",
      "  [ 3.71776373  2.5096972 ]\n",
      "  [ 3.84761212  2.5304781 ]\n",
      "  [ 4.45171243  2.42170588]\n",
      "  [ 4.90627806  2.34052896]\n",
      "  [ 4.69729335  1.99652109]\n",
      "  [ 3.77721649  2.16981819]\n",
      "  [ 3.65749824  1.98410149]\n",
      "  [ 3.39737558  1.73946472]\n",
      "  [ 2.78258082  1.78740223]\n",
      "  [ 1.24692686  2.15004323]\n",
      "  [ 1.39231153  1.4076606 ]\n",
      "  [ 0.80217619 -0.43771633]\n",
      "  [ 0.74863945 -1.67071144]\n",
      "  [ 1.46779435 -1.76509059]\n",
      "  [ 2.49322117 -2.09273438]\n",
      "  [ 2.68322926 -1.74589412]\n",
      "  [ 3.06496225 -2.22177322]\n",
      "  [ 4.40682098 -2.25135642]\n",
      "  [ 4.64045433 -2.29362204]\n",
      "  [ 5.17192661 -2.21731859]\n",
      "  [ 4.77054114 -2.50798993]\n",
      "  [ 4.39075428 -2.52099103]\n",
      "  [ 4.05277326 -2.67638959]\n",
      "  [ 3.27139767 -2.82381234]\n",
      "  [ 3.02797157 -2.76743182]\n",
      "  [ 1.27539638 -2.78536255]\n",
      "  [ 0.60650927 -2.63296025]\n",
      "  [ 1.03544342 -0.11642283]\n",
      "  [ 2.25667701 -0.10718419]\n",
      "  [ 3.34261331 -0.27396868]\n",
      "  [ 4.25005249 -0.49113701]\n",
      "  [ 4.8860616  -0.57815918]\n",
      "  [ 4.12808143 -0.47331928]\n",
      "  [ 2.23272535 -1.00300145]\n",
      "  [ 1.47498918 -0.89591745]\n",
      "  [ 0.38264526 -2.58106813]\n",
      "  [ 1.3620848   2.35091676]\n",
      "  [ 1.80422557  2.58959517]\n",
      "  [ 3.52557676  2.06397993]\n",
      "  [ 3.5445103   1.99165876]\n",
      "  [ 3.66023152  1.84410671]\n",
      "  [ 3.70011415  1.91781217]\n",
      "  [ 1.85175248  1.74289135]\n",
      "  [ 0.3213045   0.07766073]\n",
      "  [ 0.67422839 -1.54524357]\n",
      "  [ 2.31185962 -1.56634501]\n",
      "  [ 3.2140535  -1.79980566]\n",
      "  [ 3.59362858 -1.87106539]\n",
      "  [ 4.67141393 -1.9739759 ]\n",
      "  [ 3.73755521 -2.25306349]\n",
      "  [ 2.08776006 -2.21872392]\n",
      "  [ 2.3596557  -2.44084636]\n",
      "  [ 0.71625653 -2.51189745]\n",
      "  [ 0.70248208  1.38732755]\n",
      "  [ 2.08080877  0.39262379]\n",
      "  [ 3.71584123  0.54705949]\n",
      "  [ 4.79034745  0.29853949]\n",
      "  [ 4.8598159   0.27289936]\n",
      "  [ 4.02837049  0.1273423 ]\n",
      "  [ 4.36028011 -0.00614093]\n",
      "  [ 4.45425726  0.21608538]\n",
      "  [ 4.15165228 -0.28736205]\n",
      "  [ 3.4930217  -0.47237759]\n",
      "  [ 2.63333121 -0.28351879]\n",
      "  [ 3.20276779 -0.30447824]\n",
      "  [ 1.90915924 -0.54854145]\n",
      "  [ 2.49382139 -0.80616105]\n",
      "  [ 2.71872242 -0.80750069]\n",
      "  [ 2.87357719 -0.56227664]\n",
      "  [ 2.16564355 -0.69912412]\n",
      "  [ 3.39292716 -0.98003121]\n",
      "  [ 3.08313024 -1.0508792 ]\n",
      "  [ 3.1557409  -1.35999111]\n",
      "  [ 3.23189975 -1.41310206]\n",
      "  [ 2.88279254 -1.43874671]\n",
      "  [ 3.72239198 -1.63142601]\n",
      "  [ 2.88762239 -1.68845043]\n",
      "  [ 3.08443884 -1.47888572]\n",
      "  [ 3.48025055 -1.99686353]\n",
      "  [ 3.31152339 -1.76044468]\n",
      "  [ 3.20982141 -2.00622528]\n",
      "  [ 2.42880418 -2.28514266]\n",
      "  [ 2.60920895 -1.9250963 ]\n",
      "  [ 2.49562983 -2.42599701]\n",
      "  [ 3.18869089 -2.52527478]\n",
      "  [ 2.73864852 -2.64730485]\n",
      "  [ 3.21207329 -2.64662663]\n",
      "  [ 2.9654739  -2.72512092]\n",
      "  [ 2.97144931 -2.62704745]\n",
      "  [ 3.04827394 -2.97687496]\n",
      "  [ 4.42325593 -3.01837528]\n",
      "  [ 3.73964578 -2.99812185]\n",
      "  [ 3.36673756 -3.00339492]\n",
      "  [ 3.33483021  2.88282825]\n",
      "  [ 3.97632281  2.96838475]\n",
      "  [ 3.17308589  2.9478937 ]\n",
      "  [ 2.79798517  2.92610245]\n",
      "  [ 2.8156145   2.52717889]\n",
      "  [ 1.73139794  2.82298178]\n",
      "  [ 0.50295909  2.72068819]\n",
      "  [ 0.98985478 -0.7440753 ]\n",
      "  [ 1.81901531 -1.1864681 ]\n",
      "  [ 2.06029769 -1.27289467]\n",
      "  [ 3.83320607 -1.01468158]\n",
      "  [ 2.61958558 -1.0394185 ]\n",
      "  [ 4.15738675 -1.22652305]\n",
      "  [ 4.12766212 -1.30112777]\n",
      "  [ 5.2332616  -1.36660294]\n",
      "  [ 4.5597942  -1.60294606]\n",
      "  [ 3.43650553 -1.65352959]\n",
      "  [ 4.11358503 -1.7326397 ]\n",
      "  [ 2.61374477 -1.73847744]\n",
      "  [ 2.34443145 -2.01391279]\n",
      "  [ 2.49559594 -1.72094945]\n",
      "  [ 1.18780334 -2.02015285]\n",
      "  [ 0.43197647 -1.43130145]\n",
      "  [ 1.04127331  0.5471407 ]\n",
      "  [ 0.98758021  0.80199205]\n",
      "  [ 1.93919653  0.58713302]\n",
      "  [ 2.78569442  0.47394509]\n",
      "  [ 3.11789802  0.57068165]\n",
      "  [ 3.46018608  0.27039875]\n",
      "  [ 3.81498471  0.32627462]\n",
      "  [ 4.52343976  0.26251427]\n",
      "  [ 4.62511014  0.03168488]\n",
      "  [ 3.49775966 -0.01151657]\n",
      "  [ 4.65886644 -0.06402131]\n",
      "  [ 2.82114737 -0.04072908]\n",
      "  [ 2.04284842 -0.18400227]\n",
      "  [ 1.41904018 -0.84895948]\n",
      "  [ 0.78795796 -0.29561479]\n",
      "  [ 1.69710452  1.72693466]\n",
      "  [ 1.8718183   2.52995778]\n",
      "  [ 2.69846991  2.4094665 ]\n",
      "  [ 2.75283932  2.37148877]\n",
      "  [ 3.3740394   2.25661554]\n",
      "  [ 2.42921638  2.03316153]\n",
      "  [ 2.35472596  2.29276167]\n",
      "  [ 0.98128021  1.96460587]\n",
      "  [ 0.57304815  2.11086255]\n",
      "  [ 1.17289417 -1.60037359]\n",
      "  [ 2.23777388 -1.72926718]\n",
      "  [ 2.60464965 -1.79855854]\n",
      "  [ 2.78457639 -1.79258194]\n",
      "  [ 2.57098837 -1.72152497]\n",
      "  [ 1.9191964  -1.75981687]\n",
      "  [ 1.60795657 -1.75411483]\n",
      "  [ 0.47439457 -1.18997279]\n",
      "  [ 0.90246847 -0.19875407]\n",
      "  [ 1.93725229  0.53347956]\n",
      "  [ 2.68771276  0.53827394]\n",
      "  [ 3.28732527  0.58445598]\n",
      "  [ 3.01624277  0.53354369]\n",
      "  [ 3.06417046  0.5416031 ]\n",
      "  [ 1.1763177   0.44003966]\n",
      "  [ 1.01907811  0.21027982]\n",
      "  [ 0.84137005  1.11794119]\n",
      "  [ 0.81973268  2.97976045]\n",
      "  [ 1.32750119 -2.96948263]\n",
      "  [ 2.75443118  2.62664116]\n",
      "  [ 3.07722138  3.11006478]\n",
      "  [ 2.73787314  2.88797034]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_signal[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5b0477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.2229913 , -1.00620656],\n",
       "        [ 1.02854609, -0.58463834],\n",
       "        [ 1.33988282, -3.05400802],\n",
       "        ...,\n",
       "        [ 2.75443118,  2.62664116],\n",
       "        [ 3.07722138,  3.11006478],\n",
       "        [ 2.73787314,  2.88797034]],\n",
       "\n",
       "       [[ 0.03664518,  2.84340005],\n",
       "        [ 0.93725984,  3.03741696],\n",
       "        [ 1.07769295, -3.03040808],\n",
       "        ...,\n",
       "        [ 0.58892108, -0.55870694],\n",
       "        [ 1.20786625, -0.79975285],\n",
       "        [ 2.28187429, -0.82649512]],\n",
       "\n",
       "       [[ 0.05200478, -1.92108213],\n",
       "        [ 1.19081254,  1.82941536],\n",
       "        [ 1.84373096,  1.92010019],\n",
       "        ...,\n",
       "        [ 2.61126618,  1.88178862],\n",
       "        [ 2.71345309,  1.89208373],\n",
       "        [ 2.57725522,  1.96217491]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.33244008, -0.97404419],\n",
       "        [ 0.56092625, -1.18170298],\n",
       "        [ 0.17141353, -2.26017415],\n",
       "        ...,\n",
       "        [ 3.36215329, -0.58652275],\n",
       "        [ 3.62771652, -0.83170005],\n",
       "        [ 3.90762244, -1.13072922]],\n",
       "\n",
       "       [[ 0.38598102, -0.74085196],\n",
       "        [ 1.44245481, -0.69464757],\n",
       "        [ 1.75686846, -0.66253733],\n",
       "        ...,\n",
       "        [ 1.74702034,  2.26823547],\n",
       "        [ 1.60645435,  2.41622516],\n",
       "        [ 1.25740164,  2.42287796]],\n",
       "\n",
       "       [[ 0.27449298,  0.76489995],\n",
       "        [ 1.2898552 ,  0.91917044],\n",
       "        [ 1.75589375,  0.9924639 ],\n",
       "        ...,\n",
       "        [ 2.4690229 ,  2.61046548],\n",
       "        [ 2.7725193 ,  2.72024492],\n",
       "        [ 3.0128709 ,  2.69243711]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb37217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 208, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0332602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes:\n",
      "Signals: (12000, 208, 2), Codes: (12000, 13), Targets: (12000, 6)\n",
      "\n",
      "Validation shapes:\n",
      "Signals: (1500, 208, 2), Codes: (1500, 13), Targets: (1500, 6)\n",
      "\n",
      "Test shapes:\n",
      "Signals: (1500, 208, 2), Codes: (1500, 13), Targets: (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Data Splitting & Normalization\n",
    "# ---------------------------\n",
    "# First split: 80% train, 20% temp\n",
    "X_sig_train_raw, X_sig_temp_raw, X_sec_train_raw, X_sec_temp_raw, y_train_raw, y_temp_raw = train_test_split(\n",
    "    X_signal, X_secret, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% validation, 50% test\n",
    "X_sig_val_raw, X_sig_test_raw, X_sec_val_raw, X_sec_test_raw, y_val_raw, y_test_raw = train_test_split(\n",
    "    X_sig_temp_raw, X_sec_temp_raw, y_temp_raw, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Signal Normalizer (fit on training only)\n",
    "scaler_signal = StandardScaler()\n",
    "X_sig_train_flat = X_sig_train_raw.reshape(-1, 2)\n",
    "scaler_signal.fit(X_sig_train_flat)\n",
    "\n",
    "def scale_and_reshape(X_raw, scaler):\n",
    "    X_flat = X_raw.reshape(-1, 2)\n",
    "    X_scaled = scaler.transform(X_flat)\n",
    "    return X_scaled.reshape(-1, 208, 2)\n",
    "\n",
    "X_sig_train = scale_and_reshape(X_sig_train_raw, scaler_signal)\n",
    "X_sig_val = scale_and_reshape(X_sig_val_raw, scaler_signal)\n",
    "X_sig_test = scale_and_reshape(X_sig_test_raw, scaler_signal)\n",
    "\n",
    "# Secret Code Normalizer\n",
    "scaler_secret = StandardScaler()\n",
    "scaler_secret.fit(X_sec_train_raw)\n",
    "\n",
    "X_sec_train = scaler_secret.transform(X_sec_train_raw)\n",
    "X_sec_val = scaler_secret.transform(X_sec_val_raw)\n",
    "X_sec_test = scaler_secret.transform(X_sec_test_raw)\n",
    "\n",
    "# Target Normalizer\n",
    "scaler_target = StandardScaler()\n",
    "scaler_target.fit(y_train_raw)\n",
    "\n",
    "y_train = scaler_target.transform(y_train_raw)\n",
    "y_val = scaler_target.transform(y_val_raw)\n",
    "y_test = scaler_target.transform(y_test_raw)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Verification\n",
    "# --------------------------------------------------\n",
    "print(\"Training shapes:\")\n",
    "print(f\"Signals: {X_sig_train.shape}, Codes: {X_sec_train.shape}, Targets: {y_train.shape}\")\n",
    "print(\"\\nValidation shapes:\")\n",
    "print(f\"Signals: {X_sig_val.shape}, Codes: {X_sec_val.shape}, Targets: {y_val.shape}\")\n",
    "print(\"\\nTest shapes:\")\n",
    "print(f\"Signals: {X_sig_test.shape}, Codes: {X_sec_test.shape}, Targets: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38110ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " signal_input (InputLayer)   [(None, 208, 2)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 208, 64)              448       ['signal_input[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 208, 64)              256       ['conv1d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 208, 128)             24704     ['batch_normalization_29[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 208, 128)             512       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 104, 128)             0         ['batch_normalization_30[0][0]\n",
      " g1D)                                                               ']                            \n",
      "                                                                                                  \n",
      " code_input (InputLayer)     [(None, 13)]                 0         []                            \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 104, 256)             98560     ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " dense_42 (Dense)            (None, 64)                   896       ['code_input[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 104, 256)             1024      ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 64)                   256       ['dense_42[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_5 (Gl  (None, 256)                  0         ['batch_normalization_31[0][0]\n",
      " obalMaxPooling1D)                                                  ']                            \n",
      "                                                                                                  \n",
      " dense_43 (Dense)            (None, 64)                   4160      ['batch_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_41 (Dense)            (None, 256)                  65792     ['global_max_pooling1d_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 64)                   256       ['dense_43[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 256)                  0         ['dense_41[0][0]']            \n",
      "                                                                                                  \n",
      " dense_44 (Dense)            (None, 64)                   4160      ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_45 (Dense)            (None, 64)                   16448     ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " dense_46 (Dense)            (None, 64)                   4160      ['dense_44[0][0]']            \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 64)                   4160      ['dense_45[0][0]']            \n",
      "                                                                                                  \n",
      " dense_48 (Dense)            (None, 64)                   4160      ['dense_46[0][0]']            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 64)                   0         ['dense_47[0][0]',            \n",
      "                                                                     'dense_48[0][0]']            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 64)                   0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)       (None, 64)                   0         ['dense_45[0][0]',            \n",
      "                                                                     'activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 128)                  0         ['multiply_2[0][0]',          \n",
      " )                                                                   'dense_46[0][0]']            \n",
      "                                                                                                  \n",
      " dense_49 (Dense)            (None, 512)                  66048     ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 512)                  2048      ['dense_49[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_50 (Dense)            (None, 256)                  131328    ['batch_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 256)                  1024      ['dense_50[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_51 (Dense)            (None, 128)                  32896     ['batch_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_52 (Dense)            (None, 6)                    774       ['dense_51[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 464070 (1.77 MB)\n",
      "Trainable params: 461382 (1.76 MB)\n",
      "Non-trainable params: 2688 (10.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.layers import BatchNormalization, GlobalMaxPooling1D, Dropout, Embedding, Dot, Softmax\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Model Architecture\n",
    "# ---------------------------\n",
    "def cosine_annealing(epoch, lr):\n",
    "    \"\"\"Learning rate scheduler\"\"\"\n",
    "    return lr * 0.5 * (1 + math.cos(math.pi * epoch / 50))\n",
    "\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    \"\"\"Custom loss with higher weight for altitude (z) coordinates\"\"\"\n",
    "    weights = tf.constant([1.0, 1.0, 2.0, 1.0, 1.0, 2.0])  # Double weight for z-coordinates\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred) * weights)\n",
    "\n",
    "# Signal Branch (CNN)\n",
    "signal_input = Input(shape=(208, 2), name='signal_input')\n",
    "x = Conv1D(64, 3, activation='relu', padding='same')(signal_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2)(x)  # Now 104 time steps\n",
    "x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GlobalMaxPooling1D()(x)  # Reduces to (256,)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "# Secret Code Branch (Dense)\n",
    "code_input = Input(shape=(13,), name='code_input')  # Fixed shape definition\n",
    "y = Dense(64, activation='relu')(code_input)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dense(64, activation='relu')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dense(64, activation='relu')(y)  # Final shape (batch, 64)\n",
    "\n",
    "# Attention-Based Fusion (Fixed implementation)\n",
    "# Project both branches to same dimension\n",
    "x_proj = layers.Dense(64)(x)  # From 256 → 64\n",
    "y_proj = layers.Dense(64)(y)  # Ensure same dimension\n",
    "\n",
    "# Compute attention scores using additive attention\n",
    "attention_scores = layers.Add()([\n",
    "    layers.Dense(64)(x_proj), \n",
    "    layers.Dense(64)(y_proj)\n",
    "])\n",
    "attention_weights = layers.Activation('softmax')(attention_scores)\n",
    "\n",
    "# Apply attention weights to x_proj features\n",
    "context_vector = layers.Multiply()([x_proj, attention_weights])  # (None, 64)\n",
    "\n",
    "# Final merge with code features\n",
    "merged = layers.concatenate([context_vector, y_proj])  # (None, 128)\n",
    "\n",
    "# Final Layers\n",
    "z = Dense(512, activation='relu')(merged)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dense(256, activation='relu')(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "outputs = Dense(6, activation='linear')(z)\n",
    "\n",
    "# Model Compilation\n",
    "model = Model(inputs=[signal_input, code_input], outputs=outputs)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_mse,\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Reconnaissance Report (Model Summary)\n",
    "# --------------------------------------------------\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8ed786",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=20,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cosine_annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e38ff814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - 17s 163ms/step - loss: 1.4526 - mae: 0.8811 - val_loss: 1.3452 - val_mae: 0.8683 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 14s 150ms/step - loss: 1.2815 - mae: 0.8175 - val_loss: 1.2772 - val_mae: 0.8390 - lr: 9.9901e-04\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 15s 155ms/step - loss: 1.2518 - mae: 0.8070 - val_loss: 1.2562 - val_mae: 0.8223 - lr: 9.9507e-04\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 14s 152ms/step - loss: 1.2564 - mae: 0.8060 - val_loss: 1.2347 - val_mae: 0.8076 - lr: 9.8626e-04\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 15s 158ms/step - loss: 1.2365 - mae: 0.7999 - val_loss: 1.1953 - val_mae: 0.7944 - lr: 9.7077e-04\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 17s 184ms/step - loss: 1.2295 - mae: 0.7943 - val_loss: 1.1984 - val_mae: 0.7910 - lr: 9.4701e-04\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 16s 168ms/step - loss: 1.2117 - mae: 0.7895 - val_loss: 1.1885 - val_mae: 0.7838 - lr: 9.1376e-04\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 15s 157ms/step - loss: 1.2127 - mae: 0.7888 - val_loss: 1.1769 - val_mae: 0.7779 - lr: 8.7028e-04\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - 15s 158ms/step - loss: 1.2065 - mae: 0.7862 - val_loss: 1.1946 - val_mae: 0.7748 - lr: 8.1645e-04\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - 15s 157ms/step - loss: 1.2034 - mae: 0.7844 - val_loss: 1.1646 - val_mae: 0.7746 - lr: 7.5291e-04\n",
      "Epoch 11/50\n",
      "94/94 [==============================] - 14s 153ms/step - loss: 1.1961 - mae: 0.7820 - val_loss: 1.1651 - val_mae: 0.7741 - lr: 6.8101e-04\n",
      "Epoch 12/50\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 1.1946 - mae: 0.7812 - val_loss: 1.1625 - val_mae: 0.7708 - lr: 6.0287e-04\n",
      "Epoch 13/50\n",
      "94/94 [==============================] - 14s 154ms/step - loss: 1.1892 - mae: 0.7798 - val_loss: 1.1558 - val_mae: 0.7679 - lr: 5.2117e-04\n",
      "Epoch 14/50\n",
      "94/94 [==============================] - 14s 154ms/step - loss: 1.1871 - mae: 0.7796 - val_loss: 1.1566 - val_mae: 0.7701 - lr: 4.3897e-04\n",
      "Epoch 15/50\n",
      "94/94 [==============================] - 14s 146ms/step - loss: 1.1833 - mae: 0.7771 - val_loss: 1.1549 - val_mae: 0.7698 - lr: 3.5939e-04\n",
      "Epoch 16/50\n",
      "94/94 [==============================] - 14s 150ms/step - loss: 1.1807 - mae: 0.7765 - val_loss: 1.1523 - val_mae: 0.7669 - lr: 2.8532e-04\n",
      "Epoch 17/50\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 1.1776 - mae: 0.7758 - val_loss: 1.1515 - val_mae: 0.7670 - lr: 2.1910e-04\n",
      "Epoch 18/50\n",
      "94/94 [==============================] - 18s 187ms/step - loss: 1.1754 - mae: 0.7753 - val_loss: 1.1488 - val_mae: 0.7665 - lr: 1.6232e-04\n",
      "Epoch 19/50\n",
      "94/94 [==============================] - 18s 195ms/step - loss: 1.1743 - mae: 0.7749 - val_loss: 1.1490 - val_mae: 0.7672 - lr: 1.1572e-04\n",
      "Epoch 20/50\n",
      "94/94 [==============================] - 14s 153ms/step - loss: 1.1729 - mae: 0.7744 - val_loss: 1.1457 - val_mae: 0.7656 - lr: 7.9159e-05\n",
      "Epoch 21/50\n",
      "94/94 [==============================] - 15s 155ms/step - loss: 1.1717 - mae: 0.7740 - val_loss: 1.1464 - val_mae: 0.7660 - lr: 5.1810e-05\n",
      "Epoch 22/50\n",
      "94/94 [==============================] - 15s 159ms/step - loss: 1.1717 - mae: 0.7739 - val_loss: 1.1453 - val_mae: 0.7655 - lr: 3.2348e-05\n",
      "Epoch 23/50\n",
      "94/94 [==============================] - 15s 160ms/step - loss: 1.1711 - mae: 0.7737 - val_loss: 1.1456 - val_mae: 0.7657 - lr: 1.9204e-05\n",
      "Epoch 24/50\n",
      "94/94 [==============================] - 15s 157ms/step - loss: 1.1708 - mae: 0.7737 - val_loss: 1.1454 - val_mae: 0.7656 - lr: 1.0806e-05\n",
      "Epoch 25/50\n",
      "94/94 [==============================] - 15s 157ms/step - loss: 1.1702 - mae: 0.7736 - val_loss: 1.1449 - val_mae: 0.7655 - lr: 5.7421e-06\n",
      "Epoch 26/50\n",
      "94/94 [==============================] - 15s 162ms/step - loss: 1.1700 - mae: 0.7735 - val_loss: 1.1449 - val_mae: 0.7654 - lr: 2.8710e-06\n",
      "Epoch 27/50\n",
      "94/94 [==============================] - 15s 162ms/step - loss: 1.1701 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 1.3454e-06\n",
      "Epoch 28/50\n",
      "94/94 [==============================] - 15s 160ms/step - loss: 1.1702 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7655 - lr: 5.8838e-07\n",
      "Epoch 29/50\n",
      "94/94 [==============================] - 14s 145ms/step - loss: 1.1699 - mae: 0.7734 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 2.3907e-07\n",
      "Epoch 30/50\n",
      "94/94 [==============================] - 14s 150ms/step - loss: 1.1703 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 8.9806e-08\n",
      "Epoch 31/50\n",
      "94/94 [==============================] - 15s 163ms/step - loss: 1.1699 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7655 - lr: 3.1027e-08\n",
      "Epoch 32/50\n",
      "94/94 [==============================] - 15s 163ms/step - loss: 1.1699 - mae: 0.7734 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 9.8027e-09\n",
      "Epoch 33/50\n",
      "94/94 [==============================] - 15s 157ms/step - loss: 1.1700 - mae: 0.7736 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 2.8145e-09\n",
      "Epoch 34/50\n",
      "94/94 [==============================] - 17s 179ms/step - loss: 1.1702 - mae: 0.7736 - val_loss: 1.1450 - val_mae: 0.7655 - lr: 7.2929e-10\n",
      "Epoch 35/50\n",
      "94/94 [==============================] - 15s 164ms/step - loss: 1.1700 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7655 - lr: 1.6926e-10\n",
      "Epoch 36/50\n",
      "94/94 [==============================] - 15s 165ms/step - loss: 1.1697 - mae: 0.7734 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 3.4885e-11\n",
      "Epoch 37/50\n",
      "94/94 [==============================] - 14s 147ms/step - loss: 1.1698 - mae: 0.7734 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 6.3243e-12\n",
      "Epoch 38/50\n",
      "94/94 [==============================] - 14s 147ms/step - loss: 1.1702 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 9.9751e-13\n",
      "Epoch 39/50\n",
      "94/94 [==============================] - 14s 148ms/step - loss: 1.1700 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7655 - lr: 1.3518e-13\n",
      "Epoch 40/50\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 1.1699 - mae: 0.7736 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 1.5511e-14\n",
      "Epoch 41/50\n",
      "94/94 [==============================] - 15s 155ms/step - loss: 1.1702 - mae: 0.7736 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 1.4812e-15\n",
      "Epoch 42/50\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 1.1700 - mae: 0.7734 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 1.1529e-16\n",
      "Epoch 43/50\n",
      "94/94 [==============================] - 16s 168ms/step - loss: 1.1704 - mae: 0.7736 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 7.1301e-18\n",
      "Epoch 44/50\n",
      "94/94 [==============================] - 14s 150ms/step - loss: 1.1703 - mae: 0.7737 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 3.3930e-19\n",
      "Epoch 45/50\n",
      "94/94 [==============================] - 16s 168ms/step - loss: 1.1700 - mae: 0.7736 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 1.1913e-20\n",
      "Epoch 46/50\n",
      "94/94 [==============================] - 15s 160ms/step - loss: 1.1700 - mae: 0.7735 - val_loss: 1.1450 - val_mae: 0.7654 - lr: 2.9154e-22\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def add_noise(signal, noise_level=0.01):\n",
    "    return signal + np.random.normal(0, noise_level, signal.shape)\n",
    "\n",
    "# Create noisy training data\n",
    "X_sig_train_noisy = add_noise(X_sig_train)\n",
    "\n",
    "# Train the model with the updated configuration\n",
    "history = model.fit(\n",
    "    x=[X_sig_train_noisy, X_sec_train],\n",
    "    y=y_train,\n",
    "    validation_data=([X_sig_val, X_sec_val], y_val),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42798fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.1640 (Normalized)\n",
      "Test MAE: 0.7692 (Normalized)\n",
      "47/47 [==============================] - 1s 13ms/step\n",
      "Real-World MAE: 5536.95 meters\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mae = model.evaluate(  \n",
    "    [X_sig_test, X_sec_test],   \n",
    "    y_test,  \n",
    "    verbose=0  \n",
    ")  \n",
    "print(f\"Test MSE: {test_loss:.4f} (Normalized)\")  \n",
    "print(f\"Test MAE: {test_mae:.4f} (Normalized)\")  \n",
    "\n",
    "# Inverse-transform for real-world error  \n",
    "y_pred_normalized = model.predict([X_sig_test, X_sec_test])  \n",
    "y_pred_real = scaler_target.inverse_transform(y_pred_normalized)  \n",
    "y_test_real = scaler_target.inverse_transform(y_test)  \n",
    "\n",
    "# Calculate real-world MAE  \n",
    "mae_real = np.mean(np.abs(y_pred_real - y_test_real))  \n",
    "print(f\"Real-World MAE: {mae_real:.2f} meters\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
