{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1496cfd3",
   "metadata": {},
   "source": [
    "## DIFFERENCE BETWEEN MODEL 2 AND MODEL 1\n",
    "### 1. Data Preprocessing  \n",
    "| **Aspect**             | **Model 2**                          | **Model 1**                             |  \n",
    "|------------------------|----------------------------------------------------|------------------------------------------------------|  \n",
    "| **Normalization**       | ‚úÖ Correct: Split first ‚Üí normalize training data only | ‚ùå Flawed: Normalizes entire dataset before splitting |  \n",
    "| **Signal Reshaping**    | Uses `GlobalAveragePooling1D` (retains temporal trends) | Uses `Flatten()` (loses temporal structure)          |  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Architecture  \n",
    "| **Component**           | **Model 2**                                        | **Model 1**                                   |  \n",
    "|-------------------------|----------------------------------------------------|-----------------------------------------------|  \n",
    "| **CNN Branch**          | - 3 Conv1D layers (128 ‚Üí 256 ‚Üí 512)<br>- BatchNorm after each Conv<br>- GlobalAveragePooling1D | - 2 Conv1D layers (32 ‚Üí 64)<br>- MaxPooling + Flatten |  \n",
    "| **Code Branch**         | - Uses `Embedding` layer<br>- BatchNorm + L2 regularization | - Simple Dense layers (64 ‚Üí 32)              |  \n",
    "| **Fusion**              | - Deeper: Dense(512 ‚Üí 256 ‚Üí 128)<br>- L2 regularization | - Simpler: Dense(64 ‚Üí 32)                   |  \n",
    "| **Regularization**      | ‚úÖ Strong: Dropout + BatchNorm + L2                | ‚úÖ Basic: Dropout only                        |  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Training Configuration  \n",
    "| **Parameter**           | **Model 2**                                        | **Model 1**                                   |  \n",
    "|-------------------------|----------------------------------------------------|-----------------------------------------------|  \n",
    "| **Learning Rate**       | 0.0001 (safer for complex models)                 | 0.001 (riskier for deep nets)                |  \n",
    "| **Batch Size**          | 64 (better generalization)                        | Not specified (defaults to 32)               |  \n",
    "| **Early Stopping**      | Patience=20 (avoids premature stops)              | Patience=10 (may stop too early)             |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb9c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../dataset/dataset.csv')\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1.1 Signal Decoding\n",
    "# --------------------------------------------------\n",
    "def parse_signal(signal_str):\n",
    "    cleaned = signal_str.strip('[]').replace(' ', '')\n",
    "    parts = cleaned.split('),(')\n",
    "    complex_samples = []\n",
    "    for p in parts:\n",
    "        p = p.replace('(', '').replace(')', '')\n",
    "        try:\n",
    "            # Convert directly to complex number\n",
    "            complex_samples.append(complex(p))\n",
    "        except ValueError:\n",
    "            # Skip any invalid entries\n",
    "            continue\n",
    "    return np.array(complex_samples[:208])\n",
    "\n",
    "# Apply to all rows\n",
    "X_signal = np.array([parse_signal(s) for s in df['received_signal']])\n",
    "# print(X_signal[:1])\n",
    "X_signal = np.hstack([X_signal.real, X_signal.imag])  # (15000, 416)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1.2 Secret Code Handling\n",
    "# --------------------------------------------------\n",
    "def parse_secret_code(code_str):\n",
    "    return np.array([int(x) for x in code_str.strip('[]').split(', ')])\n",
    "\n",
    "X_secret = np.array([parse_secret_code(c) for c in df['secret_code']])  # (15000, 13)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1.3 Target Preparation\n",
    "# --------------------------------------------------\n",
    "y = df[['jet1_x', 'jet1_y', 'jet1_z', \n",
    "        'jet2_x', 'jet2_y', 'jet2_z']].values  # (15000, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9e3172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11931579  0.85771666 -1.33474694 -0.50391545 -2.29890648 -2.36311255\n",
      "  -1.73337179 -2.09075436 -1.68077154 -2.06851403 -2.23007812 -1.68373539\n",
      "  -0.99208445 -1.52579929 -2.13198495 -1.542111   -1.572531   -1.17894242\n",
      "  -0.43579253 -0.37435658 -0.20439189 -0.34568808 -0.3036721   0.78714854\n",
      "  -0.13114397  0.91876952  0.59695031  0.46554019  0.30731446  0.57871762\n",
      "   1.3063269   1.32965779  1.84256292  2.62761689  3.8078487   2.76445951\n",
      "   2.48637366  2.88149825  2.38199167  2.72596637  1.39522053  0.8044313\n",
      "   0.23753253 -0.8312159  -2.08004069 -2.24298221 -2.99989836 -3.15122902\n",
      "  -3.34715525 -3.41449118 -1.9398927  -2.12972652 -1.46899167 -0.57031672\n",
      "  -0.59802137 -0.68256009  0.22612962  0.72654863 -0.07467601 -0.28339311\n",
      "  -1.24302304 -0.46743044 -1.8572562  -2.77289709 -3.06969035 -3.11564192\n",
      "  -3.84457788 -3.57200481 -3.62208702 -3.10760311 -2.81848032 -1.19532471\n",
      "  -0.52973206  1.028434    2.24372655  3.21794968  3.74768431  4.09193022\n",
      "   3.67424067  1.20070218  0.9215773  -0.32409149 -0.95804282 -1.53625975\n",
      "  -1.6691226  -1.44810181 -0.98797122 -1.25838309 -0.31710668  0.32033606\n",
      "   0.01722652  0.01029079 -0.7296314  -1.06291356 -1.83280567 -2.35673147\n",
      "  -1.26003689 -1.80362926 -0.57888357  0.12816169  1.92247659  3.17354318\n",
      "   4.5784564   4.6799711   3.99575245  4.3601979   4.35067019  3.98141323\n",
      "   3.11049737  2.52820087  3.05545169  1.62905881  1.72640733  1.87947046\n",
      "   2.43117159  1.65759689  1.88984898  1.53172444  0.66033044  0.5075424\n",
      "   0.37956633 -0.2255492  -0.33895737  0.28309368 -1.43836252 -0.62426709\n",
      "  -1.35390063 -1.59116717 -0.90522305 -1.88345831 -2.60201123 -2.41085092\n",
      "  -2.82657584 -2.71199095 -2.58669605 -3.00701459 -4.3897203  -3.70122355\n",
      "  -3.33463868 -3.22380333 -3.916825   -3.11374581 -2.73327251 -2.3006711\n",
      "  -1.64425923 -0.4590607   0.72825052  0.68201515  0.60472815  2.02351268\n",
      "   1.32740075  1.40317075  1.09965867  1.06118701 -0.1465709  -0.28398909\n",
      "  -0.6628539  -0.43622468 -1.00519143 -0.37331504 -0.51596518  0.06006327\n",
      "   0.88926361  0.68664113  1.61444382  2.47864035  2.62381389  3.33445836\n",
      "   3.6137174   4.36846922  4.62278869  3.49752771  4.64932199  2.81880775\n",
      "   2.00836369  0.93765139  0.75377888 -0.2639077  -1.53247703 -2.00700008\n",
      "  -1.97609364 -2.13680349 -1.08359135 -1.55614644 -0.37652617 -0.29465723\n",
      "  -0.03468595 -0.35313954 -0.58812493 -0.61252843 -0.3860559  -0.36061119\n",
      "  -0.29311999  0.17632541  0.88470189  1.66805717  2.30765629  2.74166994\n",
      "   2.59701579  2.62563671  1.06425552  0.99663042  0.36812856 -0.80902182\n",
      "  -1.30788815 -2.39722722 -3.07569212 -2.65028829 -0.18838487 -0.56765235\n",
      "  -0.11720316 -0.81051995 -0.68204147 -0.48197614 -0.74450004  0.18309591\n",
      "   0.52066226  0.24628794  0.6992717   0.90507247  0.15223365  0.78545085\n",
      "   1.08187145  1.16016919  1.51293791  1.09127687  1.50406356  2.38332379\n",
      "   1.86631131  1.99013825  2.35893882  1.76130989  1.94498267  1.97959009\n",
      "   1.92890914  1.51874787  1.70770458  1.75845775  1.61684064  1.79340078\n",
      "   1.85777249  2.22575113  1.06173948  1.54385667  0.6689708   0.44325394\n",
      "   0.21341432 -0.66803755 -0.12616764 -0.34332671  0.32838168  0.25308863\n",
      "   1.06603248  0.77849702  2.19599112  2.20768538  2.93501197  3.52318242\n",
      "   4.27801135  3.11955596  3.34953084  3.34916402  2.71755891  1.04352208\n",
      "   1.3738256  -0.34002019 -0.74490571 -1.44017658 -2.16126018 -2.64220136\n",
      "  -2.43815361 -3.42507122 -3.48005999 -4.12814732 -2.82440855 -2.55333208\n",
      "  -1.81809151 -1.02217693 -1.10669803 -0.4447862  -0.29535984 -0.12027711\n",
      "  -0.24141722 -0.90435814 -2.00444718 -2.67015074 -1.88175762 -1.88238593\n",
      "  -1.15164593 -0.2034259   0.96820915  0.94611621  3.10543414  3.23520547\n",
      "   3.52437337  3.47955697  1.82439869  0.02492767 -0.67400828 -2.31183672\n",
      "  -3.13014024 -3.43283864 -4.29685135 -2.90088534 -1.66464684 -1.52147839\n",
      "  -0.42180236  0.69069216  0.79614629  1.93289929  1.40895908  1.30984009\n",
      "   0.51159665 -0.02677601  0.95502702 -1.17667556 -1.5893416  -0.7366367\n",
      "  -0.96017523 -0.99551814 -1.79962854 -1.96444454 -1.53194341 -1.39369456\n",
      "  -2.81787608 -2.67572651 -3.08588146 -3.19179835 -2.85769526 -3.7155524\n",
      "  -2.86765949 -3.07142002 -3.16910984 -3.25214972 -2.91031039 -1.8350141\n",
      "  -2.44714989 -1.63730054 -1.84317311 -1.29922807 -1.52574044 -1.19964185\n",
      "  -1.46236608 -0.49983721 -0.5436439  -0.53469125 -0.46379591  0.85333731\n",
      "   0.68529192  0.61078726  0.59828287  1.62314433  0.54235633  0.20550211\n",
      "  -0.67042051 -1.68631908 -1.96955082 -3.25558984 -2.25837018 -3.91343537\n",
      "  -3.97848531 -5.12453989 -4.55743789 -3.42475115 -4.05982839 -2.57708551\n",
      "  -2.11800591 -2.46751595 -1.06988631 -0.42778041  0.54171981  0.70981591\n",
      "   1.07426921  1.27139113  1.68430661  0.92427009  1.22276525  1.17387562\n",
      "   0.14652152 -0.04028132 -0.29806303 -0.11487097 -0.37377127 -1.06512202\n",
      "  -0.22955424  1.6764595   1.07481063  1.80379891  1.91655374  2.61117075\n",
      "   2.17414857  1.76724152  0.90616714  0.49148887 -1.17238118 -2.20973401\n",
      "  -2.53738228 -2.71637158 -2.54183832 -1.88501311 -1.58101392 -0.4404084\n",
      "  -0.17819067  0.98515569  1.37786876  1.81376772  1.53402393  1.5796115\n",
      "   0.50108234  0.21271581  0.75656125  0.13208086 -0.22734994  1.35653706\n",
      "   0.09700217  0.68696531]]\n"
     ]
    }
   ],
   "source": [
    "print(X_signal[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5b0477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11931579,  0.85771666, -1.33474694, ...,  1.35653706,\n",
       "         0.09700217,  0.68696531],\n",
       "       [-0.035028  , -0.9321786 , -1.07103858, ..., -0.31218129,\n",
       "        -0.86626219, -1.67845885],\n",
       "       [-0.01784629, -0.30454525, -0.63100547, ...,  2.48600477,\n",
       "         2.57460476,  2.38237365],\n",
       "       ...,\n",
       "       [ 0.18681767,  0.2127872 , -0.10902881, ..., -1.86084518,\n",
       "        -2.68116411, -3.5353162 ],\n",
       "       [ 0.28481301,  1.10820823,  1.38517485, ...,  1.33907262,\n",
       "         1.06573864,  0.82789565],\n",
       "       [ 0.19803341,  0.78227132,  0.95982124, ...,  1.25057382,\n",
       "         1.13393457,  1.30820358]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb37217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 416)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0332602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes:\n",
      "Signals: (12000, 208, 2), Codes: (12000, 13), Targets: (12000, 6)\n",
      "\n",
      "Validation shapes:\n",
      "Signals: (1500, 208, 2), Codes: (1500, 13), Targets: (1500, 6)\n",
      "\n",
      "Test shapes:\n",
      "Signals: (1500, 208, 2), Codes: (1500, 13), Targets: (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Data Loading & Initial Processing\n",
    "# --------------------------------------------------\n",
    "# [Keep your existing parsing code for X_signal, X_secret, y]\n",
    "# But REMOVE all normalization here\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Initial Splitting (RAW DATA)\n",
    "# --------------------------------------------------\n",
    "# First split: 80% train, 20% temp (using raw unnormalized data)\n",
    "X_sig_train_raw, X_sig_temp_raw, X_sec_train_raw, X_sec_temp_raw, y_train_raw, y_temp_raw = train_test_split(\n",
    "    X_signal,  # Raw (15000, 416)\n",
    "    X_secret,   # Raw (15000, 13)\n",
    "    y,          # Raw (15000, 6)\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% validation, 50% test\n",
    "X_sig_val_raw, X_sig_test_raw, X_sec_val_raw, X_sec_test_raw, y_val_raw, y_test_raw = train_test_split(\n",
    "    X_sig_temp_raw, \n",
    "    X_sec_temp_raw, \n",
    "    y_temp_raw, \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Proper Normalization (AFTER Splitting)\n",
    "# --------------------------------------------------\n",
    "# 3.1 Signal Normalization\n",
    "scaler_signal = StandardScaler()\n",
    "# Flatten training signals for proper scaling\n",
    "X_sig_train_flat = X_sig_train_raw.reshape(-1, 2)  # (12000*208, 2)\n",
    "scaler_signal.fit(X_sig_train_flat)\n",
    "\n",
    "# Transform all sets\n",
    "def scale_and_reshape(X_raw, scaler):\n",
    "    X_flat = X_raw.reshape(-1, 2)\n",
    "    X_scaled = scaler.transform(X_flat)\n",
    "    return X_scaled.reshape(-1, 208, 2)\n",
    "\n",
    "X_sig_train = scale_and_reshape(X_sig_train_raw, scaler_signal)\n",
    "X_sig_val = scale_and_reshape(X_sig_val_raw, scaler_signal)\n",
    "X_sig_test = scale_and_reshape(X_sig_test_raw, scaler_signal)\n",
    "\n",
    "# 3.2 Secret Code Normalization\n",
    "scaler_secret = StandardScaler()\n",
    "scaler_secret.fit(X_sec_train_raw)\n",
    "\n",
    "X_sec_train = scaler_secret.transform(X_sec_train_raw)\n",
    "X_sec_val = scaler_secret.transform(X_sec_val_raw)\n",
    "X_sec_test = scaler_secret.transform(X_sec_test_raw)\n",
    "\n",
    "# 3.3 Target Normalization\n",
    "scaler_target = StandardScaler()\n",
    "scaler_target.fit(y_train_raw)\n",
    "\n",
    "y_train = scaler_target.transform(y_train_raw)\n",
    "y_val = scaler_target.transform(y_val_raw)\n",
    "y_test = scaler_target.transform(y_test_raw)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Verification\n",
    "# --------------------------------------------------\n",
    "print(\"Training shapes:\")\n",
    "print(f\"Signals: {X_sig_train.shape}, Codes: {X_sec_train.shape}, Targets: {y_train.shape}\")\n",
    "print(\"\\nValidation shapes:\")\n",
    "print(f\"Signals: {X_sig_val.shape}, Codes: {X_sec_val.shape}, Targets: {y_val.shape}\")\n",
    "print(\"\\nTest shapes:\")\n",
    "print(f\"Signals: {X_sig_test.shape}, Codes: {X_sec_test.shape}, Targets: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38110ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None, 208, 2)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 208, 128)             1408      ['input_13[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 208, 128)             512       ['conv1d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPoolin  (None, 104, 128)             0         ['batch_normalization_12[0][0]\n",
      " g1D)                                                               ']                            \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 104, 256)             98560     ['max_pooling1d_8[0][0]']     \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)       [(None, 13)]                 0         []                            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 104, 256)             1024      ['conv1d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 13, 8)                8000      ['input_14[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPoolin  (None, 52, 256)              0         ['batch_normalization_13[0][0]\n",
      " g1D)                                                               ']                            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)         (None, 104)                  0         ['embedding_4[0][0]']         \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 52, 512)              393728    ['max_pooling1d_9[0][0]']     \n",
      "                                                                                                  \n",
      " dense_32 (Dense)            (None, 128)                  13440     ['flatten_4[0][0]']           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 512)                  0         ['conv1d_14[0][0]']           \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 128)                  512       ['dense_32[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_31 (Dense)            (None, 256)                  131328    ['global_average_pooling1d_4[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 128)                  0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 256)                  0         ['dense_31[0][0]']            \n",
      "                                                                                                  \n",
      " dense_33 (Dense)            (None, 64)                   8256      ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 320)                  0         ['dropout_8[0][0]',           \n",
      " )                                                                   'dense_33[0][0]']            \n",
      "                                                                                                  \n",
      " dense_34 (Dense)            (None, 512)                  164352    ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " dense_35 (Dense)            (None, 256)                  131328    ['dense_34[0][0]']            \n",
      "                                                                                                  \n",
      " dense_36 (Dense)            (None, 128)                  32896     ['dense_35[0][0]']            \n",
      "                                                                                                  \n",
      " dense_37 (Dense)            (None, 6)                    774       ['dense_36[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 986118 (3.76 MB)\n",
      "Trainable params: 985094 (3.76 MB)\n",
      "Non-trainable params: 1024 (4.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3.1 Dual Input Branches\n",
    "# --------------------------------------------------\n",
    "# Branch 1: Radar Signal Processor (CNN)\n",
    "signal_input = Input(shape=(208, 2))\n",
    "x = Conv1D(128, 5, activation='relu', padding='same')(signal_input)  # Wider kernel\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2)(x)  # 104 timesteps\n",
    "x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2)(x)  # 52 timesteps\n",
    "x = Conv1D(512, 3, activation='relu', padding='same')(x)\n",
    "x = GlobalAveragePooling1D()(x)  # Better than Flatten for temporal data\n",
    "x = Dense(256, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dropout(0.3)(x)  # Reduced dropout\n",
    "\n",
    "# Branch 2: Code Breaker (Dense Network)\n",
    "# Treat secret codes as categorical features\n",
    "code_input = Input(shape=(13,))\n",
    "y = Embedding(input_dim=1000, output_dim=8)(code_input)  # Adjust input_dim\n",
    "y = Flatten()(y)\n",
    "y = Dense(128, activation='relu', kernel_regularizer='l2')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(64, activation='relu')(y)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3.2 Combined Strike Force\n",
    "merged = concatenate([x, y])\n",
    "z = Dense(512, activation='relu')(merged)\n",
    "z = Dense(256, activation='relu')(z)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "outputs = Dense(6, activation='linear')(z)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# War Machine Assembly\n",
    "# --------------------------------------------------\n",
    "model = Model(inputs=[signal_input, code_input], outputs=outputs)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Reconnaissance Report (Model Summary)\n",
    "# --------------------------------------------------\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa803936",
   "metadata": {},
   "source": [
    "2. Global Average Pooling (GAP)\n",
    "Purpose:\n",
    "\n",
    "Summarizes each feature map by taking the average of all timesteps.\n",
    "\n",
    "Outputs one value per filter (e.g., 512 filters ‚Üí 512 values).\n",
    "\n",
    "Why Replace Flatten() with GAP:\n",
    "\n",
    "Retains temporal context: Instead of flattening into 52√ó64=3328 values (losing sequence info), GAP collapses each filter‚Äôs output into a single meaningful average.\n",
    "\n",
    "Reduces overfitting: Fewer parameters ‚Üí less risk of memorizing noise.\n",
    "\n",
    "Analogy:\n",
    "Instead of listing every radar ping‚Äôs strength over time, you report the average intensity for each frequency band.\n",
    "\n",
    "\n",
    "\n",
    "3. Batch Normalization\n",
    "Purpose:\n",
    "\n",
    "Stabilizes training by normalizing the outputs of a layer to have zero mean and unit variance.\n",
    "\n",
    "Applied after Conv/Dense layers but before activation functions.\n",
    "\n",
    "Why Add It:\n",
    "\n",
    "Faster convergence: Prevents exploding/vanishing gradients.\n",
    "\n",
    "Reduces dependency on careful weight initialization.\n",
    "\n",
    "Acts as a regularizer (reduces overfitting).\n",
    "\n",
    "Analogy:\n",
    "Like calibrating radar sensors before each mission to ensure consistent signal strength readings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af420a",
   "metadata": {},
   "source": [
    "# üçµ Tea Strainer Analogy  \n",
    "\n",
    "## 1. Radar Signal Branch (Advanced Tea Brewing)  \n",
    "**Input**: A pot of tea with:  \n",
    "- **208 tea leaves** (timesteps)  \n",
    "- **2 leaf types** (real/imaginary radar channels).  \n",
    "\n",
    "**Steps**:  \n",
    "1. **First Strainer (`Conv1D(128, 5`))**  \n",
    "   - Use **128 ultra-sensitive strainers** (filters) with **5-leaf windows** (kernel size=5).  \n",
    "   - Detects broader flavor patterns (e.g., bitter-sweet waves).  \n",
    "\n",
    "2. **Quality Control (`BatchNorm`)**  \n",
    "   - Standardizes the tea‚Äôs pH/sugar levels ‚Üí ensures consistent flavor batches.  \n",
    "\n",
    "3. **Concentrate (`MaxPooling`)**  \n",
    "   - Boil down tea to **104 leaves** ‚Üí retain strongest flavors.  \n",
    "\n",
    "4. **Second Strainer (`Conv1D(256, 3`))**  \n",
    "   - **256 precision strainers** with **3-leaf windows** ‚Üí detect subtle spice notes.  \n",
    "   - Another `BatchNorm` ‚Üí stabilize flavors.  \n",
    "   - Concentrate again ‚Üí **52 leaves**.  \n",
    "\n",
    "5. **Third Strainer (`Conv1D(512, 3`))**  \n",
    "   - **512 nano-strainers** ‚Üí extract microscopic flavor compounds.  \n",
    "\n",
    "6. **Final Taste Test (`GlobalAveragePooling1D`)**  \n",
    "   - Instead of listing all 52√ó512 flavors, take the **average intensity** of each flavor type.  \n",
    "   - Output: **512 signature tastes** (e.g., \"smoky-7\", \"sweet-42\").  \n",
    "\n",
    "7. **Blend & Reduce (`Dense(256`))**  \n",
    "   - Mix 512 tastes ‚Üí 256 elite flavors.  \n",
    "   - **Dropout(0.3)**: Randomly block 30% of flavors to prevent over-reliance.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Code Branch (Spice Lab)  \n",
    "**Input**: A **13-digit secret code** (spice recipe).  \n",
    "\n",
    "**Steps**:  \n",
    "1. **Spice Decoder (`Embedding`)**  \n",
    "   - Convert each code digit into an **8D spice vector** (e.g., digit \"5\" ‚Üí `[0.2, -0.7, ...]`).  \n",
    "   - Like grinding spices into **aromatic powders** for better mixing.  \n",
    "\n",
    "2. **Flatten & Mix**  \n",
    "   - Spread powders into a **104D spice paste** (13 digits √ó 8D ‚Üí 104 features).  \n",
    "\n",
    "3. **Refine (`Dense(128`))**  \n",
    "   - Extract **128 aromatic compounds** (e.g., cinnamon essence).  \n",
    "   - `BatchNorm` ‚Üí stabilize acidity.  \n",
    "   - **Dropout(0.2)**: Remove 20% of compounds to avoid overpowering.  \n",
    "\n",
    "4. **Final Extract (`Dense(64`))**  \n",
    "   - Condense to **64 pure spices** (e.g., \"smoky-essence-X\").  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Fusion (Master Brew)  \n",
    "1. **Combine Flavors (`concatenate`)**  \n",
    "   - Mix **256 tea flavors** + **64 spices** ‚Üí **320-dimensional super-blend**.  \n",
    "\n",
    "2. **Master Brewing (`Dense(512 ‚Üí 256 ‚Üí 128`))**  \n",
    "   - Three-step refinement:  \n",
    "     - **512** ‚Üí \"Harmonize bitter-sweet balance\"  \n",
    "     - **256** ‚Üí \"Adjust aroma intensity\"  \n",
    "     - **128** ‚Üí \"Perfect the aftertaste\"  \n",
    "\n",
    "3. **Serve (`Dense(6`))**  \n",
    "   - Pour into **6 cups** (coordinates: x,y,z for two jets).  \n",
    "\n",
    "---\n",
    "\n",
    "# üö® What the Analogy Misses  \n",
    "1. **Mathematical Precision**:  \n",
    "   - `GlobalAveragePooling1D` averages values mathematically ‚Üí no tea equivalent.  \n",
    "   - `BatchNorm` uses mean/variance normalization ‚Üí not just \"quality control\".  \n",
    "\n",
    "2. **Dynamic Learning**:  \n",
    "   - Strainers (**filters**) auto-adjust their holes (**weights**) during training.  \n",
    "   - The `Embedding` layer learns spice vectors from data, unlike fixed grinding.  \n",
    "\n",
    "3. **Regularization**:  \n",
    "   1. `L2 regularization` subtly penalizes complex recipes ‚Üí not just dropout. \n",
    "   2. Why Use It in Your Model?\n",
    "      - Prevents Overfitting:\n",
    "         Stops the model from relying too heavily on any single feature (e.g., one radar signal spike or code digit).\n",
    "\n",
    "      - Smooths Predictions:\n",
    "         Encourages smaller weights ‚Üí less \"jumpy\" predictions (critical for radar coordinate estimation).\n",
    "   3. Imagine you‚Äôre crafting a tea recipe:\n",
    "\n",
    "      - Without L2: You might use 10kg of cinnamon to mask all other flavors (overfitting to cinnamon).\n",
    "\n",
    "      - With L2: Forces you to use balanced spice quantities (smaller weights), ensuring no single flavor dominates.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "4. **Cross-Modal Fusion**:  \n",
    "   - Mixing radar signals (tea) and codes (spices) has no real-world culinary parallel.  \n",
    "\n",
    "5. **Training Nuances**:  \n",
    "   - **Lower learning rate (0.0001)**: Simmering instead of boiling ‚Üí avoids burning.  \n",
    "   - **Larger batch size (64)**: Tasting 64 teas at once ‚Üí faster, stable learning.  \n",
    "   - **Patience=20**: Waits longer for flavor perfection before stopping.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1aace45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(\n",
    "    learning_rate=0.0001,  # Reduced from 0.001\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',  # Direct metric\n",
    "    patience=20,  # Increased patience\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "286a60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 31s 159ms/step - loss: 4.6232 - mae: 0.8651 - val_loss: 3.7776 - val_mae: 0.8611\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 29s 153ms/step - loss: 3.1452 - mae: 0.8319 - val_loss: 2.6685 - val_mae: 0.8449\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 2.2529 - mae: 0.7994 - val_loss: 1.9501 - val_mae: 0.8011\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 27s 145ms/step - loss: 1.7392 - mae: 0.7936 - val_loss: 1.5469 - val_mae: 0.7902\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 30s 157ms/step - loss: 1.4255 - mae: 0.7891 - val_loss: 1.3042 - val_mae: 0.7846\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 35s 184ms/step - loss: 1.2338 - mae: 0.7865 - val_loss: 1.1577 - val_mae: 0.7826\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 33s 173ms/step - loss: 1.1122 - mae: 0.7837 - val_loss: 1.0524 - val_mae: 0.7766\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 1.0319 - mae: 0.7786 - val_loss: 0.9948 - val_mae: 0.7755\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 26s 138ms/step - loss: 0.9733 - mae: 0.7705 - val_loss: 0.9652 - val_mae: 0.7717\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 27s 142ms/step - loss: 0.9306 - mae: 0.7605 - val_loss: 0.9045 - val_mae: 0.7536\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 30s 161ms/step - loss: 0.8932 - mae: 0.7481 - val_loss: 0.8712 - val_mae: 0.7435\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 30s 161ms/step - loss: 0.8705 - mae: 0.7426 - val_loss: 0.8412 - val_mae: 0.7311\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 31s 167ms/step - loss: 0.8531 - mae: 0.7376 - val_loss: 0.8336 - val_mae: 0.7317\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 29s 156ms/step - loss: 0.8412 - mae: 0.7350 - val_loss: 0.8160 - val_mae: 0.7233\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 31s 167ms/step - loss: 0.8311 - mae: 0.7321 - val_loss: 0.8156 - val_mae: 0.7294\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 34s 179ms/step - loss: 0.8215 - mae: 0.7296 - val_loss: 0.8047 - val_mae: 0.7212\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.8144 - mae: 0.7279 - val_loss: 0.8091 - val_mae: 0.7269\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 29s 152ms/step - loss: 0.8088 - mae: 0.7264 - val_loss: 0.8040 - val_mae: 0.7257\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 29s 153ms/step - loss: 0.8032 - mae: 0.7245 - val_loss: 0.7836 - val_mae: 0.7163\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 31s 167ms/step - loss: 0.7953 - mae: 0.7222 - val_loss: 0.7787 - val_mae: 0.7154\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 30s 162ms/step - loss: 0.7925 - mae: 0.7218 - val_loss: 0.7786 - val_mae: 0.7170\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 32s 169ms/step - loss: 0.7871 - mae: 0.7197 - val_loss: 0.7711 - val_mae: 0.7107\n",
      "Epoch 23/200\n",
      "188/188 [==============================] - 29s 156ms/step - loss: 0.7796 - mae: 0.7160 - val_loss: 0.7715 - val_mae: 0.7143\n",
      "Epoch 24/200\n",
      "188/188 [==============================] - 32s 169ms/step - loss: 0.7787 - mae: 0.7166 - val_loss: 0.7710 - val_mae: 0.7120\n",
      "Epoch 25/200\n",
      "188/188 [==============================] - 35s 185ms/step - loss: 0.7726 - mae: 0.7132 - val_loss: 0.7642 - val_mae: 0.7092\n",
      "Epoch 26/200\n",
      "188/188 [==============================] - 30s 161ms/step - loss: 0.7703 - mae: 0.7129 - val_loss: 0.7683 - val_mae: 0.7106\n",
      "Epoch 27/200\n",
      "188/188 [==============================] - 35s 188ms/step - loss: 0.7651 - mae: 0.7103 - val_loss: 0.7630 - val_mae: 0.7094\n",
      "Epoch 28/200\n",
      "188/188 [==============================] - 27s 142ms/step - loss: 0.7630 - mae: 0.7094 - val_loss: 0.7607 - val_mae: 0.7079\n",
      "Epoch 29/200\n",
      "188/188 [==============================] - 26s 138ms/step - loss: 0.7589 - mae: 0.7073 - val_loss: 0.7731 - val_mae: 0.7123\n",
      "Epoch 30/200\n",
      "188/188 [==============================] - 29s 154ms/step - loss: 0.7551 - mae: 0.7058 - val_loss: 0.7644 - val_mae: 0.7067\n",
      "Epoch 31/200\n",
      "188/188 [==============================] - 28s 151ms/step - loss: 0.7525 - mae: 0.7044 - val_loss: 0.7565 - val_mae: 0.7068\n",
      "Epoch 32/200\n",
      "188/188 [==============================] - 31s 166ms/step - loss: 0.7460 - mae: 0.7018 - val_loss: 0.7640 - val_mae: 0.7097\n",
      "Epoch 33/200\n",
      "188/188 [==============================] - 35s 187ms/step - loss: 0.7442 - mae: 0.7010 - val_loss: 0.7792 - val_mae: 0.7130\n",
      "Epoch 34/200\n",
      "188/188 [==============================] - 29s 155ms/step - loss: 0.7427 - mae: 0.7002 - val_loss: 0.7523 - val_mae: 0.7014\n",
      "Epoch 35/200\n",
      "188/188 [==============================] - 27s 142ms/step - loss: 0.7426 - mae: 0.7000 - val_loss: 0.7560 - val_mae: 0.7040\n",
      "Epoch 36/200\n",
      "188/188 [==============================] - 29s 154ms/step - loss: 0.7373 - mae: 0.6976 - val_loss: 0.7556 - val_mae: 0.7031\n",
      "Epoch 37/200\n",
      "188/188 [==============================] - 31s 163ms/step - loss: 0.7333 - mae: 0.6963 - val_loss: 0.7509 - val_mae: 0.7018\n",
      "Epoch 38/200\n",
      "188/188 [==============================] - 28s 151ms/step - loss: 0.7297 - mae: 0.6948 - val_loss: 0.7552 - val_mae: 0.7021\n",
      "Epoch 39/200\n",
      "188/188 [==============================] - 35s 184ms/step - loss: 0.7288 - mae: 0.6943 - val_loss: 0.7508 - val_mae: 0.6994\n",
      "Epoch 40/200\n",
      "188/188 [==============================] - 33s 175ms/step - loss: 0.7263 - mae: 0.6941 - val_loss: 0.7563 - val_mae: 0.7013\n",
      "Epoch 41/200\n",
      "188/188 [==============================] - 32s 168ms/step - loss: 0.7249 - mae: 0.6935 - val_loss: 0.7530 - val_mae: 0.7004\n",
      "Epoch 42/200\n",
      "188/188 [==============================] - 29s 154ms/step - loss: 0.7210 - mae: 0.6915 - val_loss: 0.7693 - val_mae: 0.7082\n",
      "Epoch 43/200\n",
      "188/188 [==============================] - 30s 161ms/step - loss: 0.7198 - mae: 0.6914 - val_loss: 0.7613 - val_mae: 0.7038\n",
      "Epoch 44/200\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.7186 - mae: 0.6906 - val_loss: 0.7535 - val_mae: 0.7013\n",
      "Epoch 45/200\n",
      "188/188 [==============================] - 29s 153ms/step - loss: 0.7184 - mae: 0.6905 - val_loss: 0.7451 - val_mae: 0.6981\n",
      "Epoch 46/200\n",
      "188/188 [==============================] - 30s 162ms/step - loss: 0.7124 - mae: 0.6883 - val_loss: 0.7581 - val_mae: 0.7044\n",
      "Epoch 47/200\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.7111 - mae: 0.6878 - val_loss: 0.7450 - val_mae: 0.6976\n",
      "Epoch 48/200\n",
      "188/188 [==============================] - 31s 163ms/step - loss: 0.7093 - mae: 0.6871 - val_loss: 0.7485 - val_mae: 0.6983\n",
      "Epoch 49/200\n",
      "188/188 [==============================] - 31s 166ms/step - loss: 0.7071 - mae: 0.6859 - val_loss: 0.7422 - val_mae: 0.6967\n",
      "Epoch 50/200\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.7091 - mae: 0.6868 - val_loss: 0.7792 - val_mae: 0.7104\n",
      "Epoch 51/200\n",
      "188/188 [==============================] - 29s 156ms/step - loss: 0.7030 - mae: 0.6840 - val_loss: 0.7354 - val_mae: 0.6922\n",
      "Epoch 52/200\n",
      "188/188 [==============================] - 31s 165ms/step - loss: 0.6960 - mae: 0.6800 - val_loss: 0.7354 - val_mae: 0.6918\n",
      "Epoch 53/200\n",
      "188/188 [==============================] - 34s 182ms/step - loss: 0.6887 - mae: 0.6762 - val_loss: 0.7321 - val_mae: 0.6865\n",
      "Epoch 54/200\n",
      "188/188 [==============================] - 27s 142ms/step - loss: 0.6817 - mae: 0.6714 - val_loss: 0.7215 - val_mae: 0.6835\n",
      "Epoch 55/200\n",
      "188/188 [==============================] - 25s 134ms/step - loss: 0.6725 - mae: 0.6655 - val_loss: 0.7071 - val_mae: 0.6753\n",
      "Epoch 56/200\n",
      "188/188 [==============================] - 30s 161ms/step - loss: 0.6615 - mae: 0.6589 - val_loss: 0.7132 - val_mae: 0.6745\n",
      "Epoch 57/200\n",
      "188/188 [==============================] - 31s 165ms/step - loss: 0.6473 - mae: 0.6503 - val_loss: 0.7094 - val_mae: 0.6704\n",
      "Epoch 58/200\n",
      "188/188 [==============================] - 29s 152ms/step - loss: 0.6379 - mae: 0.6453 - val_loss: 0.6984 - val_mae: 0.6631\n",
      "Epoch 59/200\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.6303 - mae: 0.6396 - val_loss: 0.6996 - val_mae: 0.6634\n",
      "Epoch 60/200\n",
      "188/188 [==============================] - 30s 160ms/step - loss: 0.6221 - mae: 0.6354 - val_loss: 0.6852 - val_mae: 0.6571\n",
      "Epoch 61/200\n",
      "188/188 [==============================] - 26s 141ms/step - loss: 0.6140 - mae: 0.6302 - val_loss: 0.6914 - val_mae: 0.6606\n",
      "Epoch 62/200\n",
      "188/188 [==============================] - 26s 138ms/step - loss: 0.6066 - mae: 0.6267 - val_loss: 0.6883 - val_mae: 0.6563\n",
      "Epoch 63/200\n",
      "188/188 [==============================] - 28s 149ms/step - loss: 0.6025 - mae: 0.6237 - val_loss: 0.6846 - val_mae: 0.6545\n",
      "Epoch 64/200\n",
      "188/188 [==============================] - 37s 196ms/step - loss: 0.5976 - mae: 0.6206 - val_loss: 0.6764 - val_mae: 0.6504\n",
      "Epoch 65/200\n",
      "188/188 [==============================] - 29s 154ms/step - loss: 0.5946 - mae: 0.6190 - val_loss: 0.6770 - val_mae: 0.6499\n",
      "Epoch 66/200\n",
      "188/188 [==============================] - 28s 152ms/step - loss: 0.5875 - mae: 0.6156 - val_loss: 0.6720 - val_mae: 0.6488\n",
      "Epoch 67/200\n",
      "188/188 [==============================] - 28s 148ms/step - loss: 0.5852 - mae: 0.6146 - val_loss: 0.6673 - val_mae: 0.6448\n",
      "Epoch 68/200\n",
      "188/188 [==============================] - 28s 151ms/step - loss: 0.5803 - mae: 0.6121 - val_loss: 0.6783 - val_mae: 0.6492\n",
      "Epoch 69/200\n",
      "188/188 [==============================] - 27s 142ms/step - loss: 0.5779 - mae: 0.6106 - val_loss: 0.6689 - val_mae: 0.6471\n",
      "Epoch 70/200\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.5766 - mae: 0.6105 - val_loss: 0.6648 - val_mae: 0.6436\n",
      "Epoch 71/200\n",
      "188/188 [==============================] - 31s 165ms/step - loss: 0.5720 - mae: 0.6078 - val_loss: 0.6817 - val_mae: 0.6482\n",
      "Epoch 72/200\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 0.5681 - mae: 0.6059 - val_loss: 0.6620 - val_mae: 0.6425\n",
      "Epoch 73/200\n",
      "188/188 [==============================] - 31s 163ms/step - loss: 0.5662 - mae: 0.6046 - val_loss: 0.6789 - val_mae: 0.6483\n",
      "Epoch 74/200\n",
      "188/188 [==============================] - 27s 145ms/step - loss: 0.5668 - mae: 0.6045 - val_loss: 0.6644 - val_mae: 0.6427\n",
      "Epoch 75/200\n",
      "188/188 [==============================] - 29s 154ms/step - loss: 0.5647 - mae: 0.6036 - val_loss: 0.6691 - val_mae: 0.6428\n",
      "Epoch 76/200\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 0.5628 - mae: 0.6026 - val_loss: 0.6688 - val_mae: 0.6429\n",
      "Epoch 77/200\n",
      "188/188 [==============================] - 30s 161ms/step - loss: 0.5609 - mae: 0.6020 - val_loss: 0.6598 - val_mae: 0.6395\n",
      "Epoch 78/200\n",
      "188/188 [==============================] - 34s 180ms/step - loss: 0.5600 - mae: 0.6014 - val_loss: 0.6657 - val_mae: 0.6437\n",
      "Epoch 79/200\n",
      "188/188 [==============================] - 32s 168ms/step - loss: 0.5597 - mae: 0.6012 - val_loss: 0.6736 - val_mae: 0.6468\n",
      "Epoch 80/200\n",
      "188/188 [==============================] - 30s 157ms/step - loss: 0.5578 - mae: 0.6000 - val_loss: 0.6624 - val_mae: 0.6414\n",
      "Epoch 81/200\n",
      "188/188 [==============================] - 29s 157ms/step - loss: 0.5558 - mae: 0.5990 - val_loss: 0.6813 - val_mae: 0.6496\n",
      "Epoch 82/200\n",
      "188/188 [==============================] - 27s 146ms/step - loss: 0.5537 - mae: 0.5980 - val_loss: 0.6705 - val_mae: 0.6435\n",
      "Epoch 83/200\n",
      "188/188 [==============================] - 29s 155ms/step - loss: 0.5527 - mae: 0.5971 - val_loss: 0.6690 - val_mae: 0.6428\n",
      "Epoch 84/200\n",
      "188/188 [==============================] - 31s 164ms/step - loss: 0.5517 - mae: 0.5968 - val_loss: 0.6743 - val_mae: 0.6445\n",
      "Epoch 85/200\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.5522 - mae: 0.5969 - val_loss: 0.6649 - val_mae: 0.6420\n",
      "Epoch 86/200\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.5512 - mae: 0.5963 - val_loss: 0.6649 - val_mae: 0.6406\n",
      "Epoch 87/200\n",
      "188/188 [==============================] - 30s 158ms/step - loss: 0.5470 - mae: 0.5935 - val_loss: 0.6668 - val_mae: 0.6418\n",
      "Epoch 88/200\n",
      "188/188 [==============================] - 32s 172ms/step - loss: 0.5479 - mae: 0.5936 - val_loss: 0.6703 - val_mae: 0.6449\n",
      "Epoch 89/200\n",
      "188/188 [==============================] - 33s 176ms/step - loss: 0.5487 - mae: 0.5939 - val_loss: 0.6759 - val_mae: 0.6463\n",
      "Epoch 90/200\n",
      "188/188 [==============================] - 31s 166ms/step - loss: 0.5457 - mae: 0.5921 - val_loss: 0.6792 - val_mae: 0.6451\n",
      "Epoch 91/200\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 0.5452 - mae: 0.5917 - val_loss: 0.6739 - val_mae: 0.6430\n",
      "Epoch 92/200\n",
      "188/188 [==============================] - 34s 180ms/step - loss: 0.5441 - mae: 0.5904 - val_loss: 0.6793 - val_mae: 0.6471\n",
      "Epoch 93/200\n",
      "188/188 [==============================] - 34s 179ms/step - loss: 0.5439 - mae: 0.5901 - val_loss: 0.6761 - val_mae: 0.6468\n",
      "Epoch 94/200\n",
      "188/188 [==============================] - 31s 166ms/step - loss: 0.5402 - mae: 0.5881 - val_loss: 0.6757 - val_mae: 0.6447\n",
      "Epoch 95/200\n",
      "188/188 [==============================] - 25s 135ms/step - loss: 0.5378 - mae: 0.5861 - val_loss: 0.6836 - val_mae: 0.6460\n",
      "Epoch 96/200\n",
      "188/188 [==============================] - 26s 139ms/step - loss: 0.5382 - mae: 0.5862 - val_loss: 0.6761 - val_mae: 0.6449\n",
      "Epoch 97/200\n",
      "188/188 [==============================] - 29s 152ms/step - loss: 0.5358 - mae: 0.5834 - val_loss: 0.6821 - val_mae: 0.6486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_sig_train, X_sec_train],\n",
    "    y_train,\n",
    "    validation_data=([X_sig_val, X_sec_val], y_val),\n",
    "    epochs=200,  # Increased capacity\n",
    "    batch_size=64,  # Larger batch size\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42798fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.6973 (Normalized)\n",
      "Test MAE: 0.6562 (Normalized)\n",
      "47/47 [==============================] - 2s 26ms/step\n",
      "Real-World MAE: 6566.90 meters\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mae = model.evaluate(  \n",
    "    [X_sig_test, X_sec_test],   \n",
    "    y_test,  \n",
    "    verbose=0  \n",
    ")  \n",
    "print(f\"Test MSE: {test_loss:.4f} (Normalized)\")  \n",
    "print(f\"Test MAE: {test_mae:.4f} (Normalized)\")  \n",
    "\n",
    "# Inverse-transform for real-world error  \n",
    "y_pred_normalized = model.predict([X_sig_test, X_sec_test])  \n",
    "y_pred_real = scaler_target.inverse_transform(y_pred_normalized)  \n",
    "y_test_real = scaler_target.inverse_transform(y_test)  \n",
    "\n",
    "# Calculate real-world MAE  \n",
    "mae_real = np.mean(np.abs(y_pred_real - y_test_real))  \n",
    "print(f\"Real-World MAE: {mae_real:.2f} meters\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
